\documentclass{article}
\usepackage{graphicx, appendix, array, listings}
\renewcommand{\appendixpagename}{Appendix}

\graphicspath{ {./img/} }

% Title page
\title{Report Rascal Assignment}
\author{Bas van de Louw, Brent Van Handenhove}
\date{15-Jan-2022}

\begin{document}
\maketitle

\begin{table}[h!tbp]
	\caption{Team}
	\begin{tabular}{l|l}
		\hline
		Bas van de Louw      & 852361065 \\
		\hline
		Brent Van Handenhove & 852152619 \\
		\hline
	\end{tabular}
\end{table}

\begin{table}[h!tbp]
	\caption{Work division}
	\begin{tabular}{l|l}
		\hline
		Bas van de Louw      & Design, code, report \\
		\hline
		Brent Van Handenhove & Design, code, report \\
		\hline
	\end{tabular}
\end{table}

\hrulefill{}

\vfill
\clearpage

%TOC page
\tableofcontents
\vfill
\clearpage

\section{Design}

\subsection{Assumptions made}

\subsubsection{Only considering Java files}
We will only be counting Java source files. Any compiled Java code or code written in another language is ignored.

\subsubsection{What is a 'Line of Code'} \label{defining loc}
In order to calculate the total amount of code or unit size, we have to first define what a 'line of code' is.

For the purpose of this project, we decided that a line should check all the below rules for it to be considered a line of code:

\begin{itemize}
\item Not blank: Trimmed length of line is 0
\item Not a comment: Starts with //, /*, * or ends with */
\end{itemize}

A small caveat here is that in Java, lines of legitimate code may actually start with * as seen below:
\begin{lstlisting}
      int x=10;
      int y=25;
      int z=x
      *
      y;
\end{lstlisting}

However, this is code smell and quality code should not have this as it is difficult to read. For the purpose of this assignment, we assume that no non-comment lines will start with *.
In a real life scenario, as illustrated above, it may happen. As such, a more mature analysis tool should mark this kind of code as a problem to be fixed.

This also means we count import other modules as lines of code. We also count the module declaration itself as a line of code. Whether or not these are really lines of code can be debated. For the purpose of this assignment we have decided that they are, as they are necessary for the code to work.

\subsubsection{Lines of Code}
In addition to the assumptions and caveats in \ref{defining loc}, we have noticed that Eclipse tends to generate some additional files for the SmallSQL / HSQLDB projects that may affect our metrics. For example: The amount of lines of code for SmallSQL increases by roughly 2000 when this happens.
For the rest of this report, assume that we have discarded any generated files and are looking solely at the base project.

\subsubsection{Unit count}
No explicit assumptions were made about what counts as a unit. Every method within the project is counted towards the total unit count.
As such, we can define the unit count as the amount of methods within the project. This includes any abstract methods without an implementation.

\subsubsection{Unit Size}
Each unit within the project has a size. We calculate the size as the amount of lines of code within the unit that check the requirements set in \ref{defining loc}. Note that the caveats apply here as well.

\subsubsection{Unit Complexity}
Each unit within the project has a cyclomatic complexity. The complexity of unit is equal to the  sum of the complexity of each statement within the unit plus 1.

Each statement within table \ref{complexityvalues} increases the unit complexity by a value of 1.

\begin{table}[h!tbp]
	\caption{Cyclomatic Complexity Statements}
	\label{complexityvalues}
	\begin{tabular}{l|l}	
		\hline
		if(\_, \_)					&			if statement \\
		\hline
		case(\_)					&			case within a switch \\
		\hline
		do(\_, \_)					&			do loop \\
		\hline
		while(\_,\_)				&			while loop \\
		\hline
		for(\_, \_, \_)				&			for loop \\
		\hline
		foreach(\_, \_, \_)			&			foreach loop \\
		\hline
		catch(\_, \_)				&			catch statement \\
		\hline
		conditional(\_, \_, \_)		&			conditional \\
		\hline
		infix(\_, "\&\&", \_)		&			infix AND \\
		\hline
		infix(\_, "\(||\)", \_)		&			infix OR \\
		\hline
	\end{tabular}
\end{table}

\subsubsection{Duplication}
By ignoring method and argument types, code may be marked as duplicate where it does in fact make sense to have similar implementations.

\subsubsection{Test Coverage} \label{assumetests}
In order to calculate test coverage, some likely inaccurate assumptions were made.
Primarily, we do not check for any 'real' unit testing that may be present - such as JUnit - because we can neither rely on assuming one test library is used nor can we account for any unknown or proprietary test libraries. In addition, any Rascal locations that point to an outside library do not appear to be tracked.

To explain our process for calculating test coverage, we go through the following steps in pseudo code:

\begin{enumerate}
\item Create a list of every method (=unit) and its location in the project
\item Create two empty lists for 'real' methods and 'test' methods
\item For every method
	\begin{enumerate}
	\item If it contains at least 1 assert, add it to the list of test methods
	\item If it has no asserts and the access modifier is public, add it to the list of real methods
	\end{enumerate}
\item Create an empty list for 'covered' methods
\item For every method in test methods
	\begin{enumerate}
	\item Get every method called within the test method
	\item If they are not yet in the list for covered methods, add them
	\end{enumerate}
\item The amount of covered methods divided by the amount of 'real' methods is the test coverage percentage
\end{enumerate}

You may have noticed that we are only counting public methods. This is because we assume that private methods are either:

\begin{itemize}
\item Used within a public method (and thus indirectly tested)
\item Only used internally (and thus not of any interest to us)
\end{itemize}

We make an additional assumption in that we count every method call that follows the pattern \verb|'/^assert/i'| as an assert. In doing this, we implicitly assume the following:
\begin{itemize}
\item That any test method's name starts with 'assert'
\item That any other method's name does not start with 'assert'.
\end{itemize}

Lastly, we do not do a final check that every covered method is public. This is because we assume that unit tests are not within the same file or package, so they should not be able to access anything other than public methods.

\subsection{Visualizing the data}
\subsubsection{Using fine-grained views}

\section{Results}

\subsection{Metrics}
The calculated metrics for SmallSQL and HSQLDB can be found in table \ref{results}.

\begin{table}[!htb]
\caption{Metric results}
\label{results}
\begin{minipage}{.5\linewidth}
\caption{SmallSQL}
\centering
\begin{tabular}{l|r}	
		\multicolumn{2}{c}{General Data}		\\		
		\hline
		Lines of Code			&			24 850 \\
		\hline
		Number of Units			&			2 358 \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Unit Size}		\\					 
		\hline
		Simple					&			38.1 \% \\
		\hline
		Moderate				&			15.0 \% \\
		\hline
		High					&			20.5 \% \\
		\hline
		Very High				&			26.3 \% \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Unit Complexity}		\\					 
		\hline
		Simple					&			63.0 \% \\
		\hline
		Moderate				&			10.6 \% \\
		\hline
		High					&			16.9 \% \\
		\hline
		Very High				&			9.5 \% \\
		\hline		
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Duplication}		\\					 
		\hline
		Duplication				&			15.5 \% \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Unit Testing}		\\					 
		\hline
		Coverage				&			29.6 \% \\
		\hline
		Asserts					&			961 \\
		\hline
				
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Metric Scoring}		\\					 
		\hline
		Unit Size Score			&			- - \\			 
		\hline
		Volume Score			&			+ + \\			 
		\hline
		Unit Complexity Score	&			- - \\			 
		\hline
		Duplication Score		&			- \\			 
		\hline
		Unit Testing Score		&			- \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{SIG Scoring}		\\					 
		\hline
		Analyzability			&			o \\
		\hline
		Changeability			&			- \\
		\hline
		Stability				&			- \\
		\hline
		Testability				&			- - \\
		\hline
		Overall Maintainability &			- \\ 		
		\hline
\end{tabular}
\end{minipage}%    
\begin{minipage}{.5\linewidth}
\centering
\caption{HSQLDB}
\begin{tabular}{l|r}	
		\multicolumn{2}{c}{General Data}		\\		
		\hline
		Lines of Code			&			160 929 \\
		\hline
		Number of Units			&			9 424 \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Unit Size}		\\					 
		\hline
		Simple					&			19.8 \% \\
		\hline
		Moderate				&			16.8 \% \\
		\hline
		High					&			20.6 \% \\
		\hline
		Very High				&			42.9 \% \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Unit Complexity}		\\					 
		\hline
		Simple					&			59.9 \% \\
		\hline
		Moderate				&			16.4 \% \\
		\hline
		High					&			13.4 \% \\
		\hline
		Very High				&			10.4 \% \\
		\hline		
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Duplication}		\\					 
		\hline
		Duplication				&			19.6 \% \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Unit Testing}		\\					 
		\hline
		Coverage				&			6.0 \% \\
		\hline
		Asserts					&			691 \\
		\hline
				
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{Metric Scoring}		\\					 
		\hline
		Unit Size Score			&			- - \\			 
		\hline
		Volume Score			&			+ \\			 
		\hline
		Unit Complexity Score	&			- - \\			 
		\hline
		Duplication Score		&			- \\			 
		\hline
		Unit Testing Score		&			- - \\
		\hline
		
		\noalign{\vskip 4mm}    
		\multicolumn{2}{c}{SIG Scoring}		\\					 
		\hline
		Analyzability			&			- \\
		\hline
		Changeability			&			- \\
		\hline
		Stability				&			- - \\
		\hline
		Testability				&			- - \\
		\hline
		Overall Maintainability &			- \\ 		
		\hline
\end{tabular}
\end{minipage} 
\end{table}

\subsection{Interpreting the results}
\subsubsection{Lines of Code}

\subsubsection{Unit Size}
We recognize that not counting comments, while more accurate for the metric, may lead to situations where methods are overly documented. We think this may be something that should either be included in the unit size calculation somehow, or that a separate metric should be created to indicate overly documented code.

\subsubsection{Unit Complexity}
\subsubsection{Duplication}
\subsubsection{Test Coverage}


\section{Evaluation and Reflection}

\subsection{Validity}

\subsubsection{Unit Test Coverage}
Something we wanted to do but couldn't was to verify that the assert decl was inside of a test library. We could do this by checking if, for example, 'junit.org' was inside the decl path. However, it appears Rascal does not correctly handle a decl to an outside library. Instead, they are 'unknown', meaning this additional check we wanted to do was not possible at this time. This means any violations of our assumption in \ref{assumetests} are not double-checked and will skew results towards a higher coverage.

Another limitation to using our method of estimating test coverage is that 'partial tests' are not properly handled. What we mean with this is that test methods that by themselves do not contain an assert are not evaluated, even if they are called by another test method that does contain asserts. Likewise, a test method with an assert that does not itself invoke the methods it tests are not properly handled.
In other words, we do not support nested testing. This may skew results towards a lower coverage.

We can conclude that while we believe our method of calculating test coverage is at worst an educated guess and at best an acceptable estimation.

\subsection{Evaluating the visualisation}

\subsection{Cooperation}
To ease collaboration, we used git for this project.

Having worked together before, we knew each other's strengths and weaknesses. Even though we both work full-time, we constantly communicated about ideas, questions, and specific code implementations. 
In the evenings, we would each work on different parts of the code - often sending over what we were currently working on for quick feedback. This allowed us to implement features quickly while having a form of code reviews.

After we had a good foundation, we started using pull requests for further features and bug fixes. We did not do this right away as we do not believe it fruitful to complicate the pipeline so early into development. We found that this works well for us.

There were constant informal code reviews.

No specific work division was done, as neither of us had any experience with Rascal. It is our belief that on a project of smaller scope with a small team, working on a bit of everything leads to a better result.

\end{document}
